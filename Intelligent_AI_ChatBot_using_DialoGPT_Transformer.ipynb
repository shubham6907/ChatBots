{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX97zcueoT499NLHrzIohY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham6907/ChatBots/blob/main/Intelligent_AI_ChatBot_using_DialoGPT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MNaBFnRKr5J_"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers==4.22.2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint\n",
        "checkpoint = \"microsoft/DialoGPT-medium\"\n",
        "# download and cache tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side='left')\n",
        "# download and cache pre-trained model\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "# Build a ChatBot class with all necessary modules to make a complete conversation\n",
        "class ChatBot():\n",
        "    # initialize\n",
        "    def __init__(self):\n",
        "        # once chat starts, the history will be stored for chat continuity\n",
        "        self.chat_history_ids = None\n",
        "        # make input ids global to use them anywhere within the object\n",
        "        self.bot_input_ids = None\n",
        "        # a flag to check whether to end the conversation\n",
        "        self.end_chat = False\n",
        "        # greet while starting\n",
        "        self.welcome()\n",
        "\n",
        "    def welcome(self):\n",
        "        print(\"Initializing ChatBot ...\")\n",
        "        # some time to get user ready\n",
        "        time.sleep(2)\n",
        "        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n",
        "        # give time to read what has been printed\n",
        "        time.sleep(3)\n",
        "        # Greet and introduce\n",
        "        greeting = np.random.choice([\n",
        "            \"Welcome, I am ChatBot, here for your kind service\",\n",
        "            \"Hey, Great day! I am your virtual assistant\",\n",
        "            \"Hello, it's my pleasure meeting you\",\n",
        "            \"Hi, I am a ChatBot. Let's chat!\"\n",
        "        ])\n",
        "        print(\">> ChatBot:  \" + greeting)\n",
        "\n",
        "    def user_input(self):\n",
        "        # receive input from user\n",
        "        text = input(\">> User:  \")\n",
        "        # end conversation if user wishes so\n",
        "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
        "            # turn flag on\n",
        "            self.end_chat=True\n",
        "            # a closing comment\n",
        "            print('>> ChatBot:  See you soon! Bye!')\n",
        "            time.sleep(1)\n",
        "            print('\\nQuitting ChatBot ...')\n",
        "        else:\n",
        "            # continue chat, preprocess input text\n",
        "            # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "            self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n",
        "                                                       return_tensors='pt')\n",
        "\n",
        "    def bot_response(self):\n",
        "        # append the new user input tokens to the chat history\n",
        "        # if chat has already begun\n",
        "        if self.chat_history_ids is not None:\n",
        "            self.bot_input_ids = torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1)\n",
        "        else:\n",
        "            # if first entry, initialize bot_input_ids\n",
        "            self.bot_input_ids = self.new_user_input_ids\n",
        "\n",
        "        # define the new chat_history_ids based on the preceding chats\n",
        "        # generated a response while limiting the total chat history to 1000 tokens,\n",
        "        self.chat_history_ids = model.generate(self.bot_input_ids, max_length=1000, \\\n",
        "                                               pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # last ouput tokens from bot\n",
        "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # in case, bot fails to answer\n",
        "        if response == \"\":\n",
        "            response = self.random_response()\n",
        "        # print bot response\n",
        "        print('>> ChatBot:  '+ response)\n",
        "\n",
        "    # in case there is no response from model\n",
        "    def random_response(self):\n",
        "        i = -1\n",
        "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # iterate over history backwards to find the last token\n",
        "        while response == '':\n",
        "            i = i-1\n",
        "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # if it is a question, answer suitably\n",
        "        if response.strip() == '?':\n",
        "            reply = np.random.choice([\"I don't know\",\n",
        "                                     \"I am not sure\"])\n",
        "        # not a question? answer suitably\n",
        "        else:\n",
        "            reply = np.random.choice([\"Great\",\n",
        "                                      \"Fine. What's up?\",\n",
        "                                      \"Okay\"\n",
        "                                     ])\n",
        "        return reply"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZziOYS1sFrs",
        "outputId": "6c4702f9-d134-41fe-fc9a-8894c6a2f1bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build a ChatBot object\n",
        "bot = ChatBot()\n",
        "# start chatting\n",
        "while True:\n",
        "    # receive user input\n",
        "    bot.user_input()\n",
        "    # check whether to end chat\n",
        "    if bot.end_chat:\n",
        "        break\n",
        "    # output bot response\n",
        "    bot.bot_response()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j13wE2lzsMOH",
        "outputId": "ad32439d-1f0b-4728-b096-c5790ceb933d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing ChatBot ...\n",
            "Type \"bye\" or \"quit\" or \"exit\" to end chat \n",
            "\n",
            ">> ChatBot:  Hi, I am a ChatBot. Let's chat!\n",
            ">> User:  hi, how are you?\n",
            ">> ChatBot:  I'm good, how are you?\n",
            ">> User:  Great. what languages do you know?\n",
            ">> ChatBot:  I know a little bit of Java, Python, and C.\n",
            ">> User:  Great. how many software projects have you done?\n",
            ">> ChatBot:  I've done a few, but I'm not sure what they are.\n",
            ">> User:  do you have a github account?\n",
            ">> ChatBot:  I do not. I'm not sure if I have one.\n",
            ">> User:  ok. are you good at web development?\n",
            ">> ChatBot:  I'm not good at web development, but I'm good at Java, Python, and C.\n",
            ">> User:  ok. It was nice meeting you. Thanks!\n",
            ">> ChatBot:  Great\n",
            ">> User:  Bye\n",
            ">> ChatBot:  See you soon! Bye!\n",
            "\n",
            "Quitting ChatBot ...\n"
          ]
        }
      ]
    }
  ]
}